#思考 #阅读/《通信的数学理论》
Reference: [如何理解信息熵？ (baidu.com)](https://baijiahao.baidu.com/s?id=1770322828011725380&wfr=spider&for=pc)

“熵”是一个让人既熟悉又陌生的概念，熟悉是因为我们经常会在一些演讲或文章中看到它的身影，陌生，是因为我们大多数人都不知道它的真正内涵。我自己看到“熵”这个概念的时候，脑子里飘过的只有一些令人迷糊的印象，比如”==熵是用来衡量一个系统的混乱程度==”、“==孤立系统的熵（混乱程度）会自发增大（熵增原理）==”等等。最近我有幸参加了公司举办的大数据培训，期间学习了一个叫“决策树”的算法，这个算法涉及到了一个核心概念——“信息熵”。为了搞清楚这个算法的数学基础，我借此契机进一步深入地了解了熵的含义，尤其是信息熵对不确定性的度量。下面仅以此文记录自己的一些学习体会，欢迎大家一起探讨和指正。

### 01 信息量

在引入信息熵之前，我们先来了解一个概念——“信息量”。从字面理解，信息量就是对信息多少的一种度量，跟我们平时所说的运动量、工作量等概念类似，这些量我们很容易理解，比如运动量可以用人体在体育运动中所消耗的热量来度量，工作量可以用工作时间来度量。但是信息这样抽象的概念，应该如何来定量地进行度量呢？  

通常来说，如果一个事件是不确定的，那么该事件可能发生，也可能不会发生。而且，一个事件的不确定性越高，该事件发生的概率就越低。当一个概率很低的事件发生了，我们从中会接收到什么样的信息呢？一般来说，大家都会很吃惊吧，而且事件发生的概率越低，但却真实发生了，那我们就越惊讶，我们从中接收到的信息量也就越大。比如说，今年年初美国著名NBA球星科比•布莱恩特坠机身亡的事件给人带来的惊讶程度和信息量就非常大，我记得当时收到这个消息是在早上起床之前，我老婆先爬起来看了一下手机之后，突然用惊讶的语气跟我说“科比坠机遇难了”，我当时听到这个消息之后一时半会儿没反应过来，惊讶地反问老婆：“你说谁坠机？科比？怎么可能？”。然后就自己爬起来看新闻确认，再然后就是一段无穷大的惊愕和悲伤。这个事件带给我们的惊讶程度和信息量巨大无比，因为成为科比这样的人本来就是一个小概率事件，而乘坐直升机坠机也是一个小概率事件，这两个事件同时发生在科比一个人身上，根据概率的乘法原理，这是一个极小小小概率事件，但是它偏偏真实地发生在了我们的眼前，仔细想想当时这个小概率事件发生之后我们是不是从中接收到了很大的信息量？

再来看另外一个简单的例子，比如你每天都会看到太阳东升西落，这件事有没有给你带来惊讶和信息量，显然没有吧，因为太阳东升西落是必然事件，也就是概率为1，这种大概率事件没有给我们带来任何的惊讶和信息量。

从上面这些例子中，我们大概可以直观感觉到一个事件所带来的信息量跟事件本身的发生概率有关系，如果一个事件发生的概率很小，但是它却真实的发生了，那么这个事件给我们带来的信息量就大，反之，信息量就小。这是信息量与事件概率之间的定性关系，我们进一步来分析一下他们的定量关系。

Note：**事件发生概率越小，包含==信息量==越大**

==事件发生概率为p(A) 与 所含信息量 h(A) **定量关系推导**：==

我们记事件A的信息量为h(A)，概率为p(A)，根据上面的定性关系，我们知道h(A)与p(A)存在一个函数关系，假设这个函数为f，即h(A)=f(p(A))，现在我们的任务就是要来求这个具体的f。

现在我们假设有另外一个事件B，这个事件与事件A是独立的，什么叫独立的，举个简单的例子，高考成绩跟身高之间就是互相独立的，两者不存在关联关系。既然事件B跟事件A是独立的，那这两个事件给我们带来的信息总量显然应该等于两个事件的信息量之和吧，用数学公式表示就是：

![](https://pics0.baidu.com/feed/dbb44aed2e738bd45e8c2db1b13eedda257ff9c7.jpeg@f_auto?token=ed84cdecedd43c25c7ecbc572fdf418c)

（式1.1）

又根据h(A)的定义有：

![](https://pic.rmb.bdstatic.com/bjh/down/d37d900aaaaec7f36a7af0de48107d99.png?x-bce-process=image/watermark,bucket_baidu-rmb-video-cover-1,image_YmpoL25ld3MvNjUzZjZkMjRlMDJiNjdjZWU1NzEzODg0MDNhYTQ0YzQucG5n,type_RlpMYW5UaW5nSGVpU01HQg==,w_4,text_QOaVsOWtpuS4jumAmuivhg==,size_4,x_3,y_3,interval_2,color_FFFFFF,effect_softoutline,shc_000000,blr_2,align_1) （式1.2）

这个p(A,B)就是事件A与B同时发生的概率，根据概率乘法原理，互相独立的两个事件的联合概率等于各自概率的乘积，即

![](https://pics7.baidu.com/feed/060828381f30e9246bc367d75cbd040a1c95f737.jpeg@f_auto?token=ff0cfc6697687e26f2a45d54ebd981e4)

把这个式子代入上面的式1.2，再结合式1.1整理可得到：

![](https://pics5.baidu.com/feed/3b292df5e0fe99258cceb3c3241d34d38fb171a0.jpeg@f_auto?token=13629b4c3fcd3800fb3368c1be5b7704)

（式1.3）

仔细观察一下这个式子， f是不是将乘法变成了加法？能化乘法为加法的函数除了对数还能有谁？所以这个f肯定是一个对数函数，但是具体这个对数函数的底和真数是什么样的呢？我们再往下分析。

我们先换一个符号，假设事件x的信息量为h(x)，那如果这个h(x)长成这样能不能满足我们的要求呢？

![](https://pics3.baidu.com/feed/d1160924ab18972b99a52c61f67811859f510a3a.png@f_auto?token=cda830e9991a5d23c736a8a4f5fb88c0)

因为这个对数的底我们暂时还不知道，不妨先设为a，且a>1（理由：a<1时的对数写起来太丑了）。

显然这个函数满足化乘法为加法的性质，即满足式1.3，因为人家本来就是一个对数函数，显然满足。

再看看信息量与概率之间的定性关系，我们上面说了，概率越小，信息量越大，那我们这个函数能满足这个定性关系吗？因为我们假设底数a>1，所以这个对数函数是一个单调递增函数，那么当概率p(x)越小，信息量h(x)也越小，这跟我们的要求是矛盾的。

我们再进一步考虑，因为概率是满足0≤p(x)≤1的实数，那么用这个函数计算出来的信息量应该≤0的实数，也就是说对于一个概率不等于1的随机事件，我们用这个函数计算出来的信息量是负的，这显然意义不大，因为信息量不像动量、速度这些物理量是既有方向又有大小的向量，它就是一个标量，如果不能保证信息量的非负性，那这样定义出来的量毫无意义。既然这个函数计算出来的信息量是负的，那我可以让它变为正的啊，怎么变？在这个函数前面加一个负号不就可以了吗？

 ![](https://pic.rmb.bdstatic.com/bjh/down/12899ec8fbb78fee8e3617c19e16cf54.png?x-bce-process=image/watermark,bucket_baidu-rmb-video-cover-1,image_YmpoL25ld3MvNjUzZjZkMjRlMDJiNjdjZWU1NzEzODg0MDNhYTQ0YzQucG5n,type_RlpMYW5UaW5nSGVpU01HQg==,w_21,text_QOaVsOWtpuS4jumAmuivhg==,size_21,x_16,y_16,interval_2,color_FFFFFF,effect_softoutline,shc_000000,blr_2,align_1) （式1.4）

加了一个负号之后，这个函数所描述的信息量就满足了上述的定性关系（概率小、信息量大）和非负性要求了。

好，信息量与概率的函数关系确定下来了，下面就来看看这个底数a到底应该怎么选。

著名的信息论之父克劳德•香农在他的《通信的数学理论》里面说，对数底数的选择与信息度量单位的选择相对应。如果所用底数为 2，则所得到的结果可以称为二进制数位（binary digit)，或者简称为比特（bit）。也就是说如果选择2作为底数，那么信息量的单位（量纲）是一个二进制数位bit。

这个信息量的单位是什么意思？我们先类比一下，想想质量、温度这些物理量的单位是怎么来的，其实就是先找到一个参照物，再根据这个参照物来衡量其他同类物理量的大小。

比如质量，质量的单位是千克，我们先找一个参照物，将它的质量任命为1千克，然后其他的物体质量就可以根据这个参照物来衡量。

![](https://pics4.baidu.com/feed/c83d70cf3bc79f3d138d8a00ab14a71d738b29d7.jpeg@f_auto?token=c15cf30f27e7e2d92946cf668a2e0894)

同样的，信息量的单位选择也是先找到一个参照物的信息量大小，将它任命为1比特，再根据这个参照物的信息量大小来衡量其他信息的信息量大小。

我们举个例子：一个硬币有正反两面，如果我抛一枚硬币，结果是正面或反面的概率都是1/2，那么按照上面的信息量计算公式，我们来计算一下“抛一枚硬币结果为正面”这个事件（记为A）的信息量：

![](https://pics2.baidu.com/feed/0bd162d9f2d3572c6bef9a769aa6092b63d0c348.png@f_auto?token=afe8c0341926f8ae3d08528cc8025a42)

当然，抛一枚硬币结果为反面这个事件的信息量也是如此。实际上，扔硬币这种随机事件是一个二项等概率分布，也就是只有两个结果，每个结果概率都相等（1/2）。

现在我把扔1枚硬币这样的二项分布等概率事件的信息量作为参照物，并任命这个信息量为1bit，则根据信息量的计算公式（式1.4）有：

![](https://pics0.baidu.com/feed/d833c895d143ad4b72d2bc8b92b730a3a50f063a.png@f_auto?token=20ad1692ce5fd98cf204ba5bcac775bf)

所以就有a=2，即计算信息量的对数函数的底数为2。

事实上，信息量的参照物正是基于硬币正反面这样的等概率事件来选择的，因为一个硬币的正反两面跟一个二进制位所能表达的信息是一一对应的，一个二进制位可以表示0或1两种状态，跟硬币的正反面刚好一致。

有了这个信息量度量的参照物之后，我们来看看抛3枚硬币这个随机事件。这个随机事件的结果分别有以下8种可能结果（事件集）：

正正正、正正反、正反反、反反反、反正正、反反正、正反正、反正反。

每一种结果都是等概率的（1/8）。我们参照物理量的单位和测量方法，将抛3三枚硬币这个事件的每一种结果（记为B）的信息量与上面选定的信息量参照物（信息量度量单位）进行比值，则有：

![](https://pics2.baidu.com/feed/8cb1cb13495409234776dbf782edbb05b2de4959.png@f_auto?token=c9b72dde51ab87cd92d4ff640a13868c)

即抛3枚硬币这个随机事件的每个可能结果的信息量是抛1枚硬币的信息量（参照物、信息量度量单位）的3倍，也就是3bit，这样的处理方式跟物理量的方式一致，非常清晰明了，也很容易理解。

好，一个事件的信息量的定量化公式我们总算是找到了，就是这个：

![](https://pics7.baidu.com/feed/8435e5dde71190ef7514accddcaef71afcfa6071.png@f_auto?token=26490be5ec3cb2ecedb6a2e3bdac14a7)

这个公式描述的是==对于一件**已经发生**的事件，我们从这个事件中接收到了多少信息量==，如果用随机事件的角度来描述，就是对于随机事件的一个观察值（已经发生的事件），我们得到了多少信息量。==如果现在这个随机事件的结果是未知的，也就是随机事件**还没发生**，但是我们事先知道了它的概率分布，那我们能否找到一种度量，用来测量在选择事件时涉及多少种“选择”，或者输出中会有多少不确定性？也就是说，我们在判断一个即将发生的随机事件的结果时，我们面临的不确定性程度有多大？==能不能定量地进行量化？

这就是引出了信息熵的概念。

### 02 信息熵

信息论之父克劳德•香农在他著名的《通信的数学理论》中提出这样的一个问题：假定有一个可能事件集，这些事件的发生概率为，这些概率是已知的，但关于将会发生哪个事件，我们也就知道这么多了。我们能否找到一种度量，用来测量在选择事件时涉及多少种“选择”，或者输出中会有多少不确定性？

![](https://pics2.baidu.com/feed/377adab44aed2e73a21bdeff97b4cb8785d6fab0.jpeg@f_auto?token=92e6d2e20a75b7a9eede9854cf79c933)

香农正是从这个问题出发提出了信息熵的概念，这个概念的思想跟物理学上的热力学熵基本上是一样的，由于本人物理知识有限，暂时无法准确科普热力学熵的概念。因此，下面就只针对信息熵这个概念进行讨论。

香农提出的这个问题可能有点抽象，我们继续用抛硬币这个例子来解释一下。如果硬币的物理结构均匀，那么抛一次硬币得到正面和反面的概率是一样的，都是1/2。现在我让你猜一下抛一枚硬币的结果，你是不是会觉得不确定性很大，因为正反面朝上的概率都一样，不好猜啊！如果我在硬币上做一些手脚，让反面部分的重量加大，从而使得每次抛硬币的时候正面朝上的概率增大，而我做这个手脚的事也被你知道了，那么当我用这枚做了手脚的硬币再来抛一次的话，你会猜哪一面朝上？肯定是猜正面朝上吧，因为做了手脚之后正面朝上的概率增大了啊，不再是原来的等概率情况。也就是说，在我做了手脚之后，正反面不再是等概率出现，这时候抛一枚硬币会出现哪个结果的不确定性程度就降低了，不确定性程度降低了，那你在猜测哪面朝上的时候是不是就更容易作出选择了呢？这就是香农提出的那个“在选择事件时涉及多少种“选择”，或者输出中会有多少不确定性？”的问题。所以我们要找的那个度量是这样的，对于一个随机事件，如果我们要判断或者选择它究竟会发生哪个可能的结果，这时候我们面对的不确定性程度有多大？我们该怎么定量化地度量它？

这个量就是信息熵。

你仔细想想香农提出的这个问题，会不会觉得是一个很伟大的想法，我们一般人看到“不确定性程度”这种抽象的定性描述，都不会去想怎么量化它，但人家香农就能想到凡人想不到的地方，这是原创性思维的极致体现。很多原创性的想法都来自于我们看似简单和习以为常的事物，比如时间，我们每个人都很熟悉它，也没觉得它有什么问题，但人家爱因斯坦就能通过深入研究时间的本质发现了狭义相对论，并因此统一了时间和空间，石破天惊地推翻了牛顿的绝对时空观，开启了物理学史上最伟大的理论革命之一。

回到我们的正题。有些同学可能会觉得这个不确定性程度难道不就是随机事件的概率吗？如果你也有同样的疑问，说明还没有真正理解香农的问题。不确定性程度描述的是我们面对一个随机事件，如果需要判断或者选择它将会发生哪个具体的结果，那么这时候我们==面临的不确定性程度有多大（**信息熵的含义**）==，注意哦，这时候我们还不知道随机事件的具体结果。但是对于随机事件的概率，它描述的是随机事件将会发生哪个具体结果的概率，也就是说，这时候我们是假设了一个具体结果了，然后用概率来描述这个具体结果的发生可能性，而且对于求解信息熵的问题，一般我们都已经事先知道了这个随机事件的概率分布。所以不确定性程度跟随机事件的概率完全是不一样的概念，你仔细琢磨琢磨，我暂时也找不到一个更通俗更恰当的生活例子来类比。

这几天刚开始培训的时候，我也完全没有get到不确定性程度的内涵，可能这是学渣的典型表现，但是后来我通过持续不断的思考思考再思考，终于将这个概念内化为自己心中的理解框架，也就有了写这篇文章的想法。

好，信息熵的概念提出来了，那它具体的度量公式是什么呢？1948年，香农在他的《通信的数学理论》神作中给出了这个非常著名和漂亮的信息熵公式：

![](https://pics1.baidu.com/feed/80cb39dbb6fd52665960d0fbbbadfd27d60736d3.png@f_auto?token=0b21466b90ab1954879e0b9df94adb25)

这个k其实跟信息的度量单位的选择有关，如果我们选择一个二进制位（底数为2）作为单位来进行度量，那么公式就变为：

![](https://pics2.baidu.com/feed/500fd9f9d72a60591513711239815e97023bbafc.png@f_auto?token=ce47d21bd039ea3d60973b25af4d0dc0)

其中，公式左边的H(X)表示随机变量X的信息熵，而且这个随机变量X的概率分布是已经给定的，它的样本空间（可能事件集）分别为，而且每个样本点（可能事件）的概率值都已经给定的，也就是说，随机变量X的概率分布是已经给定的。

我们仔细观察这个信息熵公式，不难发现，这个信息熵跟上面的信息量存在一定的定量关系，它其实就是随机变量X每个可能事件的信息量的数学期望，相当于每个可能事件的信息量的“平均值”。哇喔，原来==信息熵是**信息量的数学期望==**！unbelievable！

虽然信息熵是信息量的数学期望，那为什么这个定义出来的信息熵就能度量我们选择随机事件将会发生哪个结果时的不确定性程度呢？也就是为什么这样定义就能满足香农提出的那个问题呢？

香农举了一个例子，比如一个随机变量X只有两种可能，概率分别为p和q=1-p，则这个随机变量的信息熵为：

![](https://pics2.baidu.com/feed/622762d0f703918fd361495f41884c9b58eec430.png@f_auto?token=17716696849b7809fea6dae4fc9549e4)

将这个信息熵作为p的函数，画出其图像：

![](https://pics7.baidu.com/feed/a5c27d1ed21b0ef4e50ef1afcd713bd683cb3e9a.jpeg@f_auto?token=1cde1a8102f525ba380eb10f52e1d513)

从这个图像可以看出，当p=0.5的时候，信息熵H(X)达到最大值，这时候随机变量X是一个等概率的二项分布，即两种可能性的概率相等。那信息熵最大说明什么？说明这时候我们判断随机变量将会发生哪个可能结果的不确定性程度最大，这个结论符合我们上面那个猜硬币的例子，也非常符合我们的直观。

香农证明了这样定义出来的信息熵H(x)具有很多性质，这些性质进一步表明了它作为选择不确定性程度的度量的合理性。我在这里截图几条重要的性质供各位读者了解，可能比较数学化，看不懂的同学可以暂时跳过，后面我再举一个具体的例子来解释一下。

![](https://pics4.baidu.com/feed/9358d109b3de9c82f091c0037c34ea061bd84382.jpeg@f_auto?token=380e386c1c952cd2cfb95197e626d790)

![](https://pics3.baidu.com/feed/962bd40735fae6cd8656f5071e06652843a70f33.jpeg@f_auto?token=901727e18b02a41be178152383dcac1e)

![](https://pics7.baidu.com/feed/267f9e2f070828387a18d02fa92cc30d4c08f131.jpeg@f_auto?token=cb25159fb6f308b6bb1ab4c90cb7a607)

我们现在来看一道关于物理学的选择题，具体题目是这样的：

下列关于力的作用效果的说法中，正确的是( )

A. 力是维持物体运动的原因

B. 力只能改变物体的运动状态

C. 力是保持物体静止的原因

D. 力能改变物体运动的快慢，也能改变物体的运动方向

看到这道题的同学们有没有想起我们中学物理课上被牛爵爷统治和支配的恐惧感？

哈哈，当然我引用这道题的用意并不是真的要来做物理题，而是为了解释信息熵的意义。

现在假设我是学渣（本来也是），我没学过牛顿力学，那么我面对这道选择题的时候，只能瞎蒙，因为对于我而言，它的答案是 ABCD 中任何一个选项的概率是一样的，都等于1/4，这时候我在选择正确答案所面临的不确定性程度最大，因为每个答案都有等可能性。也就是说随机事件“正确答案是哪个”是等概率分布，且有4个可能事件集（样本点），那么根据上面香农证明的性质，这时候的信息熵最大，也就是不确定性程度最大。

但是如果这时候有一位乐于扫盲的小伙伴偷偷告诉我A是错的，因为力不是维持物体运动的原因，那我在选择正确答案的过程中就获得了一个新的信息输入，这样我就可以先排除掉A选项，然后在剩下的BCD三个选项中再蒙一个。其实这时候随机事件“正确答案是哪个”的概率分布已经发生变化，即随机事件的可能事件集减少为3个，且每个可能事件的概率都为1/3。所以，这时候我在选择正确答案时所面临的不确定性程度较之前显然是有所下降了，也就是说这时候的信息熵下降了。那信息熵具体下降了多少呢？我们不妨来算一下。

在我获得小伙伴的信息之前，设表示随机事件“正确答案是哪个”，那么根据信息熵的计算公式可得：

![](https://pics2.baidu.com/feed/cb8065380cd791234d5cb7d3bd81338eb3b7801e.png@f_auto?token=e25b00583b6a03672f2844a9b6ebc30f)

即我在选择正确答案时所面临的不确定性程度为2。

当我获得了小伙伴的信息输入之后，设表示这时候的随机事件“正确答案是哪个”，同样根据信息熵的计算公式可得：

![](https://pics1.baidu.com/feed/1e30e924b899a9016e94efc00d2060770008f5e0.png@f_auto?token=26abf6e120477c7f43e97571cb71559b)

显然，因此，在获得小伙伴的信息输入之后，我在选择正确答案时所面临的不确定性程度下降了。你看，通过这个信息熵的定义之后，我们可以将选择时不确定性程度量化为具体的数字，而且结果非常符合直观感觉和定性关系。

我们再作进一步的思考，为什么信息熵会下降？因为有个小伙伴偷偷告诉了我一个事件信息啊，然后我就得以排除掉A选项，从而调整了随机事件的概率分布，最终使得信息熵下降了。为什么信息熵的改变量会是这个数值？我们抛开信息熵的计算公式从另外一个角度来分析。

小伙伴偷偷告诉我的那个事件信息：“A选项是错的”，这个事件已经发生了，那它的概率是多少呢？我们都知道，ABCD四个选项，每个选项是正确答案的概率都是1/4，也就是说”每个选项是错的”这些事件的概率都是3/4。根据上面的信息量计算公式，小伙伴偷偷告诉我的那个事件（记为x）的信息量就等于：

![](https://pics5.baidu.com/feed/faf2b2119313b07e7a5659f51c62fb2f95dd8ca1.png@f_auto?token=10c300e2af2785dfb0e90e0baf694428)

咦，好巧哦！怎么跟上面计算出来的信息熵改变量相等啊？

因为==信息熵的**改变量**就是等于信息量==！

到这里，我们终于可以来给信息量和信息熵下一个总结了：

1、==信息熵是不确定性程度的一个度量==，==而信息量就是信息熵的改变量==，它是用来消除不确定性的。我向你发出一条信息，这条信息究竟有多大的信息量，在于它能帮助你消除多少不确定性。

2、==信息熵发生变化本质上都是因为随机事件概率分布的变化引起的==，这个变化包含两层含义：第一层含义是随机事件的可能事件集（样本空间）不变，但是可能事件集的概率分配（概率值）发生了变化；第二层含义是随机事件的可能事件集（样本空间）和概率分配都发生了变化。

3、==信息熵成功地量化了信息和随机事件选择的**不确定性程度==**，为通信技术的发展奠定了坚实的数学基础。

以上就是我个人对信息熵的一些理解和体会，欢迎大家指错交流，也希望对大家有所帮助。

### 03 结语

事实上，信息量和信息熵仅仅是香农[[《通信的数学理论》]]中的基础性概念，这部划时代的神作的内容远不止于此，它开创了信息论，将人类带入了信息时代。香农洞察了信息的本质，提出了信息度量的方法和基本单位，找到了提高信息传输质量的办法。可以说，现代通讯的基本特征就是通讯数字化，而通讯数字化的理论基础就是香农的《通信的数学理论》这部信息论神作。今天你能看到的一切现代通讯工具和通讯方式，从互联网到移动网，本质上都是在传输数字信息，这些都是信息论的应用成果。现在我们每一天的生活，都已经很难摆脱电脑、手机和互联网，而这些都和香农的贡献息息相关。

![](https://pics2.baidu.com/feed/37d12f2eb9389b50aa5cf60797808fd1e6116e01.jpeg@f_auto?token=ee0c9b69210b6a3f128d62b6e0d7deeb)

所以，在本文的最后，请允许我再吹一波香农大神：

克劳德•香农是伟大的信息论之父，在科学史上，他是足以与牛顿、爱因斯坦相提并论的天才。他让计算机从运算工具变成了思考工具，奠定了现代计算机理论的基础，并且首先抓住了信息的本质，催生了各种现代通讯工具，奠定了互联网的基础。今天的IT人士，都应该叫香农一声“祖师爷”。