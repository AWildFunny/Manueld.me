#机器学习/支持向量机SVM 
[[支持向量机SVM|SVM]]的变种：**最小二乘（LS）** 支持向量机LSSVM
- 通过最小化一个简化的目标函数和平方误差损失来解决回归问题
- 它使用核函数映射到高维空间，并求解一个线性系统，而不是像SVM一样解决一个凸优化问题。
- LS-SVM在数学上更直接和通常更容易计算，尤其是在小到中等规模的数据集上
- 这种方法可能对数据中的噪声更敏感，因为**所有的数据点（而不只是[[支持向量机SVM#^99fb76|支持向量]]）** 都会影响到最终的模型。

___
==以下为与SVM的对比==
### 目的：
- SVM：找到最大化数据点间隔的超平面。
- LS-SVM：找到一个能够拟合数据的超平面，同时最小化输出误差的平方和。

### 核函数与空间映射：
- SVM：使用核函数将数据映射到高维空间，以解决非线性可分问题。
- LS-SVM：同样使用核函数映射，但目标是回归问题，寻找最佳拟合超平面而不是最大化间隔。

### 数学表述与优化问题：
- 线性可分SVM：
  - 超平面定义：$\mathbf{w} \cdot \mathbf{x} + b = 0$。
  - 优化目标：$\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2$。
  - 约束条件：$y_i(\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1$。

- LS-SVM：
  - 回归模型：$f(\mathbf{x}_i) = \mathbf{w}^T \phi(\mathbf{x}_i) + b$。
  - 优化目标：$\min_{\mathbf{w}, b, e_i} \frac{1}{2} \|\mathbf{w}\|^2 + \frac{1}{2} C \sum_{i=1}^N e_i^2$。
  - 约束条件：$y_i = \mathbf{w}^T \phi(\mathbf{x}_i) + b + e_i$。

### 松弛因子与损失函数：
- 线性SVM：引入松弛因子来处理数据点位于间隔内的情况，适用于“软间隔”优化。
- LS-SVM：使用平方损失函数，使得每个数据点（而不仅是支持向量）都直接影响模型，更适用于回归问题。

### 核心公式：
- SVM对偶问题涉及不等式约束，核心是找到支持向量并最大化间隔。
- LS-SVM对偶问题通过等式约束简化，核心是求解一个线性系统以最小化预测误差。

### 回归问题（SVR vs LS-SVM）：
- SVR（支持向量回归）：对于每个不在ε边界内的数据点引入松弛变量，目标是找到一个在ε容忍度内最适合数据的函数。
- LS-SVM：同样处理回归，但通过最小化所有数据点的平方误差来求解，使得优化问题成为线性方程组。