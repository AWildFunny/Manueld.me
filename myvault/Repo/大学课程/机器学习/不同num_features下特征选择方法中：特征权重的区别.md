`highest_weights`方法和`forward_selection`方法在考虑特征重要性时采取不同方法：

### Highest Weights 方法
- 首先通过训练一个全局线性模型，得到**每个特征的系数（全局重要性）**
- 然后通过将这些**系数与特定实例的特征值（data[0]：标准化邻域数据的第一条，即待解释实例）相乘**，得到新的数据权重，以反映它们在局部的重要性

*关键假设：**特征的重要性不仅取决于其全局系数的大小，还取决于在特定实例中该特征的值** *
>也就是说：
>即使某个特征的全局系数很大，当[[待解释特征（即data[0]）很小或为零时]]，对该实例的预测贡献也可能微乎其微。
---

### Forward Selection 方法
- 通过线性模型的迭代过程，直接评估每个特征的贡献
- 在每一步，它都会尝试添加一个新特征，并评估这个新特征是否能显著提高模型的性能（R²）

*通过**实际的模型性能改进**来直接评估特征的重要性，可以自然地考虑特征之间的相互作用和非线性效应，而不仅仅是单个特征与响应变量之间的线性关系。*

---

### 方法选择的原因

- **计算效率**：`highest_weights`方法适用于特征数量较多的情况，因为它通过简单的乘法操作快速评估特征的局部重要性，避免了在每次迭代中重新训练模型的计算成本。
- **局部解释的需求**：`highest_weights`方法通过考虑局部上下文（即特定实例的特征值）来提供对单个预测的解释，这与LIME的目标一致——提供局部而不是全局解释。
- **模型的假设**：`highest_weights`方法基于线性模型系数的解释性，这适用于线性或近似线性的局部模型。而`forward_selection`方法不依赖于这种线性假设，因此在处理复杂模型和特征相互作用时更为灵活。
