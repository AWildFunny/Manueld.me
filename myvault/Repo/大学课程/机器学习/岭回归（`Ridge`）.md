
岭回归（Ridge Regression）是一种用于多重回归的技术，**特别是当数据点少于变量数**，或当各变量高度相关（多重共线性）时。岭回归通过对回归系数的大小施加惩罚来解决普通最小二乘法（OLS）在这些情况下可能出现的问题（如**过拟合**）。以下是岭回归的基本原理和为什么要在特定情况下使用它：

### 岭回归目标

- **自变量**：各个特征的值
- **因变量**：在例子（解释表格数据）中，表示 *被预测为标签类型的概率P*

![[240118-岭回归（`Ridge`）-1.png]]

**损失函数越小，说明拟合出的线性模型（直线）与原数据点越接近**

### 岭回归的原理

1. **正则化**:
   - 岭回归在普通最小二乘回归的基础上添加了L2正则化项。这意味着在最小化残差平方和的同时，还要最小化回归系数的平方和。
   - 公式可以表示为：\(\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - X_i\beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}\)，其中 \(\lambda\) 是正则化参数。

2. **正则化参数 \(\lambda\)**:
   - \(\lambda\) 控制着正则化的强度。较大的 \(\lambda\) 提供更强的收缩，使得回归系数更接近于零。
   - 当 \(\lambda = 0\) 时，岭回归等同于普通最小二乘回归。

3. **目的**:
   - 通过正则化，岭回归减少了模型的复杂性，防止过拟合。这在变量之间存在多重共线性的情况下尤其有用。

4. **收缩系数**:
   - 岭回归通过“收缩”系数向零，减少了每个变量对预测的影响，特别是对于相关变量。



总之，岭回归在处理具有多重共线性的数据集或需要正则化以防止过拟合的情况下非常有用。在像 LIME 这样的算法中，岭回归被用来生成局部的、可解释的模型，帮助我们理解复杂模型在特定实例上的预测行为。