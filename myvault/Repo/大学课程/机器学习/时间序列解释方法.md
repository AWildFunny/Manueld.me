#### 事后解释Post-hoc Interpretation Methods
- 针对CNN
- 最初都用于计算机视觉领域
- 反向传播法:通过反向传播计算输入或中间特征对输出的贡献度,代表性方法有CAM、Gradient*Input、LRP等
- 扰动法:通过观察输入扰动前后输出变化来解释模型,如ConvTimeNet中使用的Occlusion Sensitivity
- 近似法:用可解释的局部模型近似被解释的黑盒模型,如LIME、SHAP

#### 自解释 Inherently Interpretable Models
- 针对RNN/其它
- Attention:通过注意力权重揭示时间步或变量的重要性,可用于CNN、RNN、Transformer等模型
- Prototype Learning:学习一组原型样本作为每一类的代表,并通过样本与原型的相似性来解释分类结果,如Shapelet等

---

- 数据挖掘法:基于符号表征(SAX)和模糊逻辑的方法
- 基于实例的解释:用有代表性的样本(如Shapelet)来解释模型行为

4. Explanation Scale

- 局部解释:针对单个样本给出解释,如CNN事后解释法通常生成局部解释
- 全局解释:针对整个数据集给出解释,如Shapelet、SAX等基于数据挖掘的方法

---
 6. Future Directions and Challenges
- 现有方法大多关注输入解释,缺乏对中间特征的分析
- 解释结果往往局限于突出显著特征,尚不足以全面理解模型内部机制
- 未来需要开发针对时间序列特点的解释方法,同时加强人机交互,使解释更加直观易懂

1. Discussion
- 当前专门面向时间序列的CNN可解释性方法较少,未来值得深入研究
- XAI方法不仅要提高可信性,还应该增强模型的置信度、稳定性和鲁棒性
- 未来XAI系统需要加强人机交互,提供个性化、易于理解的解释,建立用户信任

