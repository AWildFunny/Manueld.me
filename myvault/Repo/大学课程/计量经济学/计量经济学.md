计量经济学，Econometrics，是运用**概率统计方法**对经济变量之间的**(因果)关系**进行定量分析的科学。

> 计量经济学侧重于解释（寻找因果），预测侧重于结果（推断未来），而前者是后者获得更高可靠性的基础。

# 数据科学基础
计量经济学的通用研究方法，来源于数据科学的数学方法。这些数学方法构成了计量经济学研究的基础。
## 经济数据的类型
1. 横截面（Cross-sectional）数据： 多个**不同**经济个体、**同一**时间点
2. 时间序列（Time Series）数据： **同一**经济个体、**不同**时间点
3. 面板（Panel）数据：多个**不同**经济个体、**不同**时间点

## 始于高数——最优化方法与微积分
在建模不同变量间的关系时，我们使用**最优化（Optimization）方法**，根据已有经济数据求出各个变量之间的参数，诸如：
- 最小二乘法（最小化问题）
- 最大似然估计（最大化问题）

为了求解最优化问题，上述方法基于微积分原理：
### 一元最优化问题
- 取**最值**的必要条件：$f'(x)=0$（一阶导数，即切线斜率为0），又称**一阶条件**（First  Order  Condition）
- 取**最大/最小值**的条件：$f''(x)<=0$ / $f''(x)>=0$（二阶导数，即曲率），又称**二阶条件**（Second  Order  Condition）
### 多元最优化问题
- 取最值的必要条件：f(x)对所有变量的偏导数均=0

## 继于线代——显著性检验的基础
最优化方法能得到参数，那么相关变量间的路径关系是否显著？由此引入沃尔德检验以检验其显著性。**二次型（Quadratic Form）**构成了沃尔德检验的基础，对于多个变量$x_1$,$x_2$,$x_3$,...$x_n$：
- 一般写法中，其二次齐次多项式函数写作：$a_{11}x_1^2 + a_{12}x_1x_2 + ... + a_{1n}x_1x_n + a_{21}x_2x_1 + a_{22}x_2^2 + ... + a_{2n}x_2x_n + ... + a_{n1}x_nx_1 + a_{n2}x_nx_2 + ... + a_{nn}x_n^2$
- 线代中，其可写作：$x'Ax$，其中$x$为n维列向量，$A$为n阶对称矩阵
### 一维二次型
当维度n=1时：
- **一维二次型**$ax^2$即为通过原点的抛物线（称为正定或负定）
- 那么，对于任何一个变量x，为了**衡量x与原点之间的距离**有几个标准差，常用的形式是： $$\frac{x^2}{\text{Var}(x)} = \left(\frac{x}{\sqrt{\text{Var}(x)}}\right)^2$$
- 其中，方差$\text{Var}(X)$/标准差$\sqrt{\text{Var}(x)}$定义见下文
### 多维二次型
当维度n>1时：
- **多维二次型**$x'Ax$可视为向量x的**加权平方和**，矩阵A给予每一项不同的权重
- 那么，对于任何一个n维列向量x，采用与上述类似的方式，**衡量向量x与零向量0的距离**
- 这个二次型值越大，说明观察到的**差异越显著**，这是沃尔德检验等方法判断显著性的基础
## 终于概统——度量变量之间的关系
### 基础：随机变量分布的数学特征
为描述随机变量X分布的**集中趋势**，以其概率为权重做加权平均，得到**期望**（Expectation）$E(X)$=$\mu$
- 对于离散型变量：
  $$E(X) = \sum_{i} x_i P(X=x_i)$$
- 对于连续型变量：
  $$E(X) = \int_{-\infty}^{\infty} x f(x) dx$$
进一步地，为描述随机变量X距离其平均值$E(X)$的**波动程度**，定义了**方差**（Variance）$\text{Var}(X) = \sigma^2$（概率论中，记为$D(X)$）。其定义式为：
$$ \text{Var}(X) \equiv \sigma^2 \equiv E[(X - E(X))^2] $$
- 取其平方根，记作**标准差**$\sigma$

>常用运算性质：
>1. 期望的线性性（linearity）：常数可提$E(aX+b) = aE(X)$+b、多项可拆$E(X+Y) = E(X) + E(Y)$
>2. 方差的非线性性：常数平方$Var(aX+b) = a^2 Var(X)$、多项当且仅当XY不相关或独立时可拆
>3. 期望与方差的关系【重要】：$Var(X) = E(X^2) - [E(X)]^2$

### 延伸方向A：单个变量的进一步描述
由期望、方差引申，可得各阶**矩**(moment)的概念。
首先是**原点矩**（距原点）：
- 一阶原点矩$E(X)$：即**期望**
- 二阶原点矩$E(X^2 )$ ...

其次是**中心矩**（距数据分布中心）：
- 一阶中心矩 $E[X - E(X)] = 0$。
- 二阶中心矩 $E[X - E(X)]^2 = E[(X - \mu)]^2$：即**方差** $\sigma^2$，度量分布的离散程度
- 三阶中心矩 $E[X - E(X)]^3 =E[(X-\mu)]^3$： 度量分布的**不对称性**
- 四阶中心矩 $E[X - E(X)]^4 =E[(X-\mu)]^4$ ：度量分布的**尾部厚度/峰态**

然而，高阶中心矩的取值受变量单位的影响。类似于标准差，对中心矩进行标准化 $Z = (X - \mu) / \sigma$（除以标准差），再求其期望，由此得到：

- 三阶中心矩$E\left[\frac{(X - \mu)}{\sigma}\right]^3$：称为**偏度 (Skewness)**
- 四阶中心矩$E\left[\frac{(X - \mu)}{\sigma}\right]^4$：称为**峰度 (Kurtosis)**

>1. 完全对称的分布：偏度=0
>2. 标准正态分布N：偏度=0，峰度=3
>3. 峰度↑ => 尾部厚度↑ & 峰顶尖度↑ => 更容易出现**极端值**（outliers）
 
峰度>3（如t分布），则其相对正态分布更尖、更厚。由此，正态分布的峰度可作为判断其它分布的基准，因此记**超额峰度**(excess   kurtosis)为 $E\left[\frac{(X - \mu)}{\sigma}\right]^4-3$


### 延伸方向B：多个变量间的关系

单个变量X的分布通过期望、方差来描述，当考虑多个变量X、Y之间的相关性时，则引入**协方差**(Covariance)的概念：
$$ \text{Cov}(X,Y) \equiv \sigma_{XY} \equiv E[(X - E(X))(Y - E(Y))] $$
- $\text{Cov}(X,Y)$ ≠ 0 => X、Y**相关**（correlated）（随机变量X 的取值大于(小于)其期望E(X)时，随机变量Y 的取值也倾向于大于(小于)其期望值E(Y)）
	- $\text{Cov}(X,Y)$ > 0 => X、Y**正相关**
	- $\text{Cov}(X,Y)$ < 0 => X、Y**负相关**
- $\text{Cov}(X,Y)$ = 0 => X、Y**线性不相关**（uncorrelated）

>协方差与方差，满足常用运算性质：
>1. 协方差的非线性性：同前 + 多变量可加$Cov(X, Y + Z) = Cov(X, Y) + Cov(X, Z)$
>2. 协方差与期望之间的关系【重要】：$Cov(X, Y) = E(XY) - E(X) E(Y)$
>3. 方差与协方差之间的关系【重要】：$Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$

然而，协方差受X 与Y计量单位的影响，无法直接通过取平方根来标准化。因此，类似于标准差，对协方差进行标准化得到**相关系数**(correlation)：
$$ P \equiv \text{Corr}(X,Y) \equiv \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}} \equiv \frac{\sigma_{XY}}{\sigma_X\sigma_Y} $$
>根据性质，可推出$P∈[-1,1]$

协方差、相关系数用来度量变量间的线性相关关系。进一步地，如果任意X的信息都无法改变Y的均值预期，那么称Y**均值独立**（mean-independent）于X。其数学表达为Y关于X的**条件期望**（Conditional Expectation）等于Y的**无条件期望**（Unconditional Expectation）：

$$ E(Y|X) = E(Y) $$
>均值独立关系**不具备对称性**：即Y均值独立于X 不能推出 X均值独立于Y
>但可以证明：Y均值独立于X => X、Y线性不相关

协方差、相关系数度量线性关系强度，均值独立衡量是否存在均值依赖关系。一个更强的概念是统计独立性，若X的取值与Y完全不相关，则称X和Y**相互独立**(independent)，充要条件是：
$$ f(x,y) = f_X(x)f_Y(y) $$

即联合密度等于边缘密度的乘积，两个变量不存在任何关系。

以上三个概念的强度关系为：
**相互独立**($f(x,y) = f_X(x)f_Y(y)$) $\implies$ **均值独立**($E(Y|X) = E(Y)$) $\implies$ **线性不相关** ($Cov(X,Y)=0$)

### 延伸方向C：统计上的常见分布
计量经济学中常用的几个抽样分布均与正态分布相关：

| 分布类型              | 正态分布 (Normal)                                                                                                                  | 卡方分布 (Chi-squared / $\chi^2$)                                                                       | t 分布 (Student's t)                                                                  | F 分布 (F-distribution)                                                                                             |
| :---------------- | :----------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------- |
| **英文名称**          | Normal Distribution                                                                                                            | Chi-squared Distribution                                                                            | Student's t-Distribution                                                            | F-Distribution                                                                                                    |
| **定义 **           | $X \sim N(\mu, \sigma^2)$ <br>$f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ <br> 标准正态: $Z \sim N(0, 1)$ | $Y = \sum_{i=1}^k Z_i^2 \sim \chi^2(k)$ 其中：$Z_1,..,Z_k \sim N(0,1)$<br> 当k=1时: $Z^2 \sim \chi^2(1)$ | $T = \frac{Z}{\sqrt{Y/k}} \sim t(k)$其中：$Z \sim N(0,1)$, $Y \sim \chi^2(k)$，且Z与Y相互独立 | $F = \frac{Y_1/k_1}{Y_2/k_2} \sim F(k_1, k_2)$其中： $Y_1 \sim \chi^2(k_1)$, $Y_2 \sim \chi^2(k_2)$, 且$Y_1, Y_2$相互独立 |
| **参数 **           | 均值 $\mu$, 方差 $\sigma^2$                                                                                                        | 自由度 (degree of freedom) $k$：代表卡方分布由$k$个独立同分布的N组成                                                    | 自由度 (d.f.) $k$                                                                      | 分子自由度 $k_1$, 分母自由度 $k_2$                                                                                          |
| **期望 (E)**        | $\mu$ ($N(0,1)$为0)                                                                                                             | $k$                                                                                                 | 0 (需 $k>1$)                                                                         | $\frac{k_2}{k_2-2}$ (需 $k_2>2$)                                                                                   |
| **方差 (Var)**      | $\sigma^2$ ($N(0,1)$为1)                                                                                                        | $2k$                                                                                                | $\frac{k}{k-2}$ (需 $k>2$)                                                           | 较复杂 (需 $k_2>4$)                                                                                                   |
| **偏度 (Skewness)** | 0                                                                                                                              | $\sqrt{8/k}$ (正偏)                                                                                   | 0 (需 $k>3$)                                                                         | 正偏                                                                                                                |
| **峰度 (Kurtosis)** | 3 ($N(0,1)$为3)                                                                                                                 | $3 + 12/k$                                                                                          | $3 + \frac{6}{k-4}$ (需 $k>4$)                                                       | 依赖 $k_1, k_2$                                                                                                     |
| **关键特征/关系**       | 对称钟形, 基础分布                                                                                                                     | 正值, 右偏; $k \to \infty$ 时趋近正态                                                                        | 对称钟形, 厚尾 (比正态); $k \to \infty$ 时趋近 $N(0,1)$                                         | 正值, 右偏; $T \sim t(k) \implies T^2 \sim F(1, k)$                                                                   |

>拓展性质： 
## 归总——统计学思想
上述数学知识为统计推断提供了基础，提供了从经济数据到数学模型的桥梁。然而，经济数据->数学模型->实证结论之间，最后一个桥梁是**统计推断**(statistical inference)的思想。

### 从总体到估计量

令研究中的每个对象为**个体**(individual)，则全部对象为**总体**(population)。统计推断的思想即是：从总体中**抽样**，即随机抽取（独立同分布）部分个体作为**样本**(sample)，由此**估计**（estimate）总体中的未知参数。

记总体中的未知参数（parameter）为**待估参数**$\theta$ （例如总体均值 $\mu$、总体方差 $\sigma^2$）。为了估计 $\theta$，根据样本数据 $(X_1, ..., X_n)$ 得到**估计量**（Estimator） $\hat{\theta}$，其具体值称为**估计值**（Estimate）。

>估计量是随机变量，而估计值是代入公式得到的具体值

### 从估计量到其优化方法

由于估计量 $\hat{\theta}$ 是一个随机变量，它通常不会精确等于待估参数 $\theta$。其间的差异可以分解为两个部分：

1.  估计量在总体平均上高估或低估真实参数，则会导致**（系统性）偏差** (Bias)：$$ \text{Bias}(\hat{\theta}) = E(\hat{\theta}) - \theta $$
- 如果 $E(\hat{\theta}) = \theta$，即**偏差为0**，则称 $\hat{\theta}$ 是 $\theta$ 的**无偏估计量** (Unbiased Estimator)，反之则为**有偏估计**(biased estimator)
- 在不同的估计量之间，偏差$\text{Bias}(\hat{\theta})$描述其对总体平均的估计准确程度，而**方差**$\text{Var}(\hat{\theta})$描述其估计的波动程度。相同**偏差**下，**方差**$\text{Var}(\hat{\theta})$越小，则称该估计越**有效**。

2.  由于抽样的随机性，不同的样本会产生**抽样误差 (Sampling Error)**： $$\hat{\theta} - \theta$$
- 为了采用前述的最优化方法来估计参数，采用**误差平方**(squared error)：$(\hat{\theta} - \theta)^2$ 来衡量不同样本抽样误差的大小

现在，上述两种误差分别反映了总体、各个样本的误差，为了搭建两者的桥梁，使得总体优化目标兼顾两者，引入误差平方的均值，即**均方误差MSE**(Mean Squared Error)：$$MSE(\hat{\theta}) = E [ (\hat{\theta} - \theta)^2 ]$$
可以证明，它同时取决于**方差**（衡量估计量围绕其均值的波动性）和**偏差的平方**（衡量估计量均值偏离真值的程度），即：
$$ MSE(\hat{\theta}) = Var(\hat{\theta}) + [Bias(\hat{\theta})]^2 $$
因此，均方误差能够作为样本估计总体时，估计参数的**最优化目标**。

>MSE的最优化中：存在**偏差-方差权衡 (Bias-Variance Tradeoff)**：有时为了降低方差，可能需要接受一定的偏差；反之亦然。理想的估计量是在两者之间取得良好平衡，使得MSE最小化。

### 参数优化之外：假设检验方法
除了估计出参数，我们还要确定参数估计的路径是否**显著**，由此引入假设检验方法。

比如：当我们对x与y之间的关系进行建模时，根据相关理论，作出**假设**：假设变量x对y产生正向影响。然后，根据回归方法进行建模、OLS估计得到未知参数，对假设进行**检验**，判断其为真或为假。

然而，根据建模结果，做出的判断不一定是准确的，可能存在两种类型的错误：
1. **第Ⅰ类错误**(Type error Ⅰ )：原假设为真，但**拒绝**了原假设（"弃真"）
2. **第ⅠⅠ类错误**(Type error ⅠⅠ )：原假设为假，但**接受**了原假设（"存伪"）

>对于两种错误，其发生的概率：
>1. 是此消彼长的
>2. 两者的发生概率之和为定值，该值只能通过改变样本容量来改变

为衡量判断的准确程度，即两类错误发生的概率，由于两者此消彼长，我们只需要测量其中一个的概率。因此，我们取第Ⅰ类错误的发生概率，并将其定义为检验的**显著性水平** $\alpha$ (Significance Level)，即 ：
$$ P(\text{拒绝} H_0 | H_0 \text{为真}) = P(\text{检验统计量落入拒绝域} | H_0 \text{为真}) = \alpha $$
它代表了我们能够容忍犯"弃真"错误的最大概率。通常，我们会预设一个较小的 $\alpha$ 值（如 0.05 或 0.01），以此作为判断是否拒绝原假设的标准。这个决策过程通常通过比较计算出的 **P 值 (p-value)**（将在后文阐述） 与预设的 $\alpha$ 来完成：

*   当 **P 值 > $\alpha$** 时：**无法拒绝**原假设 $H_0$，认为（在该显著性水平下）原假设 $H_0$为真
*   当 **P 值 ≤ $\alpha$** 时：**拒绝**原假设 $H_0$，认为（在该显著性水平下）原假设 $H_0$为假

该假设检验方法，能够用于判断建模和参数估计的结果是否显著。
# 一元线性回归
对于具体的定量问题(quantitative question)，计量经济学研究不同变量间的关系，则需要通过回归的方式建立联系、求解方程。由此引入最简单的一元线性回归。

## 引入：线性回归模型

为研究多个变量之间的关系，记：
- 我们试图解释或预测的变量 $y_i$ 为 **被解释变量** (dependent variable, regressand)
- 用来解释 $y_i$ 变化的变量 $x_i$ 为 **解释变量** (explanatory variable, independent variable, regressor) 

为衡量它们之间的关系，按照前述统计推断思想，我们从总体随机抽取 $n$ 个个体，则一元线性回归模型可写为：
$$ y_i = \alpha + \beta x_i + \epsilon_i \quad (i=1, ..., n) $$
其中：
- $\beta$：**斜率** (slope) ，表示 $x_i$ 每变化一个单位， $y_i$ 平均变化多少个单位，最直接衡量两者之间的关系
- $\alpha$：**截距项** (intercept) 或 **常数项** (constant) ，补充方程使得估计更准确
- $\epsilon_i$: **"误差项"** (error term) 或 **"扰动项"** (disturbance) ，是未被模型考虑在内的估计误差

在该方程中，$\alpha$ 与 $\beta$ 统称为 **"回归系数"** (regression coefficients) 或 **"参数"** (parameters)，是我们需要根据样本数据 $(x_i, y_i)$ 估计的总体未知参数。

## 参数估计方法：普通最小二乘法OLS
如何根据样本数据 $(x_i, y_i)$ 估计总体模型 $y_i = \alpha + \beta x_i + \epsilon_i$ 中的未知参数 $\alpha$ 和 $\beta$？如前文所述，我们希望找到能使 MSE 最小的估计量 $\hat{\theta}$（即 $\hat{\alpha}$ 和 $\hat{\beta}$）。

然而，在实际应用中，我们无法直接计算和最小化$MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2]$，因为：
1.  真实的总体参数 $\theta$（即 $\alpha$ 和 $\beta$）是未知的
2.  计算期望 $E[\cdot]$ 需要的估计量 $\hat{\theta}$ 的抽样分布是未知的

为了不直接最小化$MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2]$，我们需要一个基于样本数据的方法来估计参数：
- 线性回归模型中，每个样本 $(x_i, y_i)$ 带有**实际观测值 (observed value)** $y_i$ 
- 将给定样本 $(x_i, y_i)$ 代入线性回归模型的方程，与参数的估计值 $\hat{\alpha}$ 、$\hat{\beta}$ 结合，能产生**拟合值 (fitted value)** $\hat{y}_i$  
- 因此，两者之间的误差称为**残差 (residual)**：
$$ e_i = y_i - \hat{y}_i = y_i - (\hat{\alpha} + \hat{\beta} x_i) $$

因此，类比于MSE，我们通过**最小化残差平方和** (Sum of Squared Residuals, SSR)来估计参数，即目标函数为：
$$ \min_{\hat{\alpha}, \hat{\beta}} SSR(\hat{\alpha}, \hat{\beta}) = \min_{\hat{\alpha}, \hat{\beta}} \sum_{i=1}^n e_i^2 = \min_{\hat{\alpha}, \hat{\beta}} \sum_{i=1}^n (y_i - \hat{\alpha} - \hat{\beta} x_i)^2 $$

这种方法称为**普通最小二乘法**(Ordinary Least Squares, OLS) 。

>残差 $e_i$ 可以看作是：对线性回归模型中未考虑在内的误差项$\epsilon_i = y_i - (\alpha + \beta x_i)$ 的一种估计或近似

由此，将问题转化为了多元最优化问题。目标函数取最小值点时，满足一阶条件：
$$
\begin{cases}
\frac{\partial}{\partial \hat{\alpha}} \sum_{i=1}^n e_i^2 = 0 \\
\\
\frac{\partial}{\partial \hat{\beta}} \sum_{i=1}^n e_i^2 = 0
\end{cases}
$$
解得：
- $$\hat{\beta} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{\sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}}{\sum_{i=1}^n x_i^2 - n \bar{x}^2}$$
- $$\hat{\alpha} = \bar{y} - \hat{\beta} \bar{x}$$
>可以发现：该**样本回归线**(sample regression line)一定经过均值点 ( $\bar{x}$ , $\bar{y}$ )

由此，我们完成了从样本数据到参数估计值的跨越。

## OLS估计下：线性回归模型的解释效果评估

根据OLS的一阶条件，可以推出：

在 $n$ 维样本空间中，**残差**$e = (e_1, ..., e_n)'$ 与下列向量均**正交**：
- **常数项** $(1, ..., 1)'$
- **解释变量** $x = (x_1, ..., x_n)'$ 
- **拟合值** $\hat{y} = (\hat{y}_1, ..., \hat{y}_n)'$ 

而**被解释变量** $y_i$ 则可以被分解为拟合值与残差两个相互正交的部分，称为**平方和分解 (Sum of Squares Decomposition)**：
$$ y_i = \hat{y}_i + e_i $$

进一步地，为了与样本的分布产生联系，将上式两边同时减去样本**均值** $\bar{y}$。为排除负数影响，对两边平方后对所有样本求和，由此得到等式左为被解释变量 $y$ 的**方差**，在OLS估计中被称为**离差平方和TSS**(Total Sum of Squares)。

该公式将TSS分解为等式右的两部分，称作是**平方和分解公式**：
$$ \underbrace{\sum_{i=1}^n (y_i - \bar{y})^2}_{\text{TSS}} = \underbrace{\sum_{i=1}^n (\hat{y}_i - \bar{y})^2}_{\text{ESS}} + \underbrace{\sum_{i=1}^n e_i^2}_{\text{RSS}} $$
等式右的两部分分别是：
1.  **解释平方和ESS** (Explained Sum of Squares)：即拟合值与均值的差异，代表线性回归模型能够解释的部 分
2.  **残差平方和RSS/SSR**(Sum of Squared Residuals)：即拟合值与真实值的差异（$e_i = y_i - \hat{y}_i$），代表线性回归模型无法解释的部分，正是前述OLS的目标函数
    $$ SSR = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 $$
>由此，平方和分解公式可以简洁地写为：$$ TSS = ESS + SSR $$$$总变异 = 被解释的变异 + 未被解释的变异$$

在衡量线性模型的解释效果时，我们认为：模型可以解释的部分所占比重（$\frac{ESS}{TSS}$）越大，效果越好。

因此，将平方和分解等式两边同时除以TSS，我们可以定义一个衡量模型**拟合优度**(goodness of fit)的指标，又为 **可决系数** (Coefficient of Determination) 或 **R 方 ($R^2$)**：
$$ R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS} $$

>$R^2∈[0,1]$ ，越大代表解释效果越好

由此，我们能够使用一元线性模型建立不同变量间的关系，使用OLS估计未知参数，并衡量其解释效果。
# 多元线性回归
当需要考虑多个**解释变量** ($x$) 对**同一个被解释变量** ($y$) 的影响时，一元线性回归模型可以扩展为多元线性回归模型。

| 类型     | 被解释变量/因变量（$y$） | 解释变量/自变量（$x$） |
| ------ | -------------- | ------------- |
| 一元线性回归 | 1个             | 1个            |
| 多元线性回归 | 1个             | **多个**        |

一般的多元回归模型可写为：
$$ y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_K x_{iK} + \epsilon_i \quad (i=1, \dots, n) $$
其中 $x_{ik}$ 为第 $i$ 个个体的第 $k$ 个解释变量（共 $K$ 个解释变量），$\beta_k$ 为第 $k$ 个解释变量对应的系数。

## 多元回归的参数估计与评估

与一元回归类似：多元回归模型的参数（$\beta_1, \dots, \beta_K$）通常也可通过**普通最小二乘法 (OLS)** 进行估计，其核心思想仍然是最小化**残差平方和 (SSR)**。

与一元回归中不同的是：多元情况下，通常使用矩阵形式 ($\hat{\beta} = (X'X)^{-1}X'y$) 进行表达和计算，其基本原理是相同的。

类似地，评估模型**拟合优度**的指标 $R^2$ 也适用于多元回归，但随着多元回归中解释变量数目的增加， $R^2$ 只会不断增加，因为纳入解释的变量越来越多了。然而，解释变量过多时，模型会出现不够简洁的问题。

为解决这一问题，将模型的简洁性（变量数目）纳入评估指标中，我们引入**校正拟合优度** (adjusted $R^2$)，记为 $\bar{R}^2$，以 $K$ 代表解释变量的个数：

$$ \bar{R}^2 = 1-\frac{\sum_{i=1}^n e_i^2/(n-K)}{\sum_{i=1}^n (y_i-\bar{y})^2/(n-1)} $$

这一公式通过 $\sum_{i=1}^n e_i^2$ 的分母，即自由度 (degree of freedom)  $(n-K)$ 对指标进行调整，从而避免了简单添加变量就能提高拟合优度的问题。

## 小样本OLS的共同性质

前述一元与多元线性回归，在小样本的研究中，得到的OLS估计量都具有共同的性质。这些性质是进行统计推断（如假设检验）的基础。

要使得这些性质成立，需要满足**古典线性回归模型 (Classical Linear Regression Model, CLRM)** 的一系列核心假定：
1. **线性性 (Linearity)**: 模型 $y = X\beta + \epsilon$ 对**参数** $\beta$ 是线性的
2.  **严格外生性 (Strict Exogeneity)**: $E(\epsilon | X) = 0$，扰动项 $\epsilon$ 的条件期望为零，即它与**所有**解释变量的**所有**观测值都不相关（这是推导无偏性的关键）
3.  **无完全多重共线性 (No Perfect Multicollinearity)**: 数据矩阵 $X$ 列满秩。即解释变量之间不存在精确的线性关系，保证 OLS 估计量可以唯一确定
4.  **球型扰动项 (Spherical Errors)**: 扰动项满足**同方差** ($Var(\epsilon_i|X) = \sigma^2$ 对所有 $i$) 和**无自相关** ($Cov(\epsilon_i, \epsilon_j|X) = 0$ 对所有 $i \neq j$)，即所有扰动项具有相同的方差，且彼此不相关

在这些古典假定下，这些假定确保了 OLS 估计量具有如下的理想性质：
1. **无偏性 (Unbiasedness)**: $E(\hat{\beta}) = \beta$。在重复抽样中，OLS 估计量的平均值等于真实的总体参数
2. **有效性 (Efficiency) - BLUE**: **高斯-马尔可夫定理 (Gauss-Markov Theorem)** 指出，在所有线性无偏估计量中，OLS 估计量具有最小的方差。因此，OLS 是**最佳线性无偏估计量 (Best Linear Unbiased Estimator, BLUE)**

这些良好的性质构成了后续进行假设检验、构建置信区间等统计推断的基础。如果这些假定不满足（例如存在异方差、自相关或内生性），OLS 估计量可能失去无偏性或有效性，需要采用更高级的估计方法。

# 大样本OLS
前文提到，小样本OLS满足古典线性回归模型的一系列假设的前提下，具备诸多优良性质。然而，在实际应用中出现了如下的问题：
1. 严格外生性假设要求解释变量与所有的扰动项均正交，然而时间序列的**自回归模型**不可能满足这一要求
2. 小样本理论假定扰动项为正态分布，然而实际应用中并无把握经济变量是否服从**正态分布**

然而，当样本容量达到一定数目时，大数定律与中心极限定理可以起作用，从而简化上述假设。因此，"**大样本理论**"(large sample theory)，也称"渐近理论"(asymptotic theory)，成为了当代计量实践中的常用方法。

>一般认为：相比于"小样本"，"大样本"的样本容量至少要≥30，最好在100以上

## 大样本OLS的数学基础
前文提到，大样本之所以能够简化上述假设，是因为大数定律与中心极限定理可以起作用。这里对相关数学基础予以补充。
### 概率收敛
当样本量 $n$ 趋于无穷大时，我们使用**收敛** (convergence) 来描述随机变量序列 $\{X_n\}$ 的极限行为。简单来说，它刻画了当 $n$ 越来越大时，随机变量 $X_n$ 是否以及如何"接近"某个确定的常数 $a$ 或另一个随机变量 $X$。

当样本量趋近于无穷大时，$\lim_{n \to \infty} P(|X_n - a| > \epsilon) = 0$ （或 $\lim_{n \to \infty} P(|X_n - X| > \epsilon) = 0$），则称随机变量 $X_n$ **依概率收敛 (Convergence in Probability)** 于常数 $a$ (或随机变量 $X$)， 记作 $X_n \xrightarrow{p} a$ （或 $X_n \xrightarrow{p} X$）

>其中： $\epsilon$ 为任意给定的很小的正数
    
进一步地，当样本量趋近于无穷大时，$\lim_{n \to \infty} E[(X_n - a)^2] = 0$，则称随机变量 $X_n$ **依均方收敛** (Convergence in Mean Square)于常数 $a$ ，记作 $X_n \xrightarrow{ms} a$

>均方收敛意味着不仅 $X_n$ 靠近 $a$，而且其波动的幅度（以均方误差衡量）也趋于零，即收敛程度更强了

此外，若随机变量 $X_n$ 的**累积分布函数 (Cumulative Distribution Function, CDF)**  $F_n(x)$ ，和另一个随机变量$X$的累积分布函数$F(x)$ ，满足$\lim_{n \to \infty} F_n(x) = F(x)$，则称随机变量 $X_n$**依分布收敛** (Convergence in Distribution)于随机变量 $X$ ，记作 $X_n \xrightarrow{d} X$

>依分布收敛只是分布函数的收敛(随机变量之间可以毫无关系)， 而依概率收敛才是随机变量本身的收敛

三者的关系是：
$$
依均方收敛 \implies 依概率收敛 \implies依分布收敛
$$
这三种收敛概念是大样本 OLS 理论的基石，帮助我们理解估计量在大样本下的行为（如一致性）以及如何进行渐近推断。

### 两大定律
前述的收敛概念，尤其是依概率收敛和依分布收敛，能够进一步导出如下定律：
1. 对于**独立同分布**的随机序列 $\{x_n\}_{n=1}^\infty$ ，其样本均值 $\bar{x}_n = \frac{1}{n}\sum_{i=1}^n x_i$ **依概率收敛**于总体均值 $\mu$。即：当样本容量n足够大时，**样本均值**趋进于**总体均值**，故名**大数定律 **(Law of Large Numbers, LLN)。
2. 对于**独立同分布**的随机序列 $\{x_n\}_{n=1}^\infty$ ，其标准化后的样本均值 $\bar{x}_n = \frac{1}{n}\sum_{i=1}^n x_i$ **依分布收敛**于标准正态分布：$$ \frac{\bar{x}_n - \mu}{\sqrt{\sigma^2/n}} \xrightarrow{d} N(0, 1) $$即：标准化(即减去期望，除以标准差)之后的**样本均值**的渐近分布为**标准正态**，故名**中心极限定理** (Central Limit Theorem, CLT)

大数定律保证了估计量在大样本下会收敛到真实值（一致性），而中心极限定理则提供了在大样本下进行统计推断的分布基础，从而导出下一小节的结论。

### 大样本OLS的共同性质
基于大数定律和中心极限定理，即使放宽小样本下的一些严格假定（如误差正态性、严格外生性），OLS 估计量在大样本下依然拥有良好的**渐近性质 (asymptotic properties)**，这些性质是进行统计推断的基础：

1.  **一致性 (Consistency)**：当样本容量 $n$ 趋于无穷大时，OLS 估计量 $\hat{\beta}_n$ 会**依概率收敛**到真实的总体参数 $\beta$。换句话说，只要样本足够大，估计量就非常接近真实值。
2.  **渐近正态性 (Asymptotic Normality)**：当样本容量 $n$ 趋于无穷大时，经过适当标准化后的 OLS 估计量 $\hat{\beta}_n$ 的**分布**会趋近于正态分布。即：$$ \sqrt{n}(\hat{\beta}_n - \beta) \xrightarrow{d} N(0, \sigma^2) $$称估计量 $\hat{\beta}_n$ 是**渐近正态 (asymptotically normal)** 的，其中 $\sigma^2$ 称为 $\hat{\beta}_n$ 的**渐近方差 (asymptotic variance)**，记为 $Avar(\sqrt{n}(\hat{\beta}_n - \beta)) = \sigma^2$，或者有时也简化记为 $Avar(\hat{\beta}_n) = \sigma^2/n$。

>在所有渐近正态的估计量中，**渐近方差最小**的估计量被称为渐近有效估计量。即：若 $Avar(\hat{\beta}_n) \le Avar(\tilde{\beta}_n)$，则称 $\hat{\beta}_n$ 比 $\tilde{\beta}_n$ 更**渐近有效 (asymptotically more efficient)**

进一步，可以导出**渐近独立定理**：如果随机序列 $\{x_i\}_{i=1}^\infty$ 是渐近独立的，那么其**函数** $y_i = f(x_i)$ 构成的序列 $\{y_i\}_{i=1}^\infty$ 也是渐近独立的。

这个命题（结合大数定律的推广）的一个关键推论是：对于这类过程，总体的任意"矩"（如 $E[f(x_i)]$）都可以通过其对应的样本矩（$\frac{1}{n}\sum_{i=1}^n f(x_i)$）进行**一致估计**。这为许多基于矩估计的方法（如 GMM）提供了理论基础。

 总的来说，大样本OLS为导出以上的渐近性质，需要满足如下的前提假设：
 1. 线性性
 2. 渐近独立的平稳过程：所有变量是**渐近独立**（其统计特性不随时间变化，并且其观测值之间的依赖性随时间距离的**增加**而**减弱**）的，故能适用两大定律
 3. 前定解释变量(predetermined regressors)：解释变量均与同期(同方程)的扰动项正交
 4. 秩条件(rank condition)：数据矩阵X满列秩，故不存在多重共线性

大样本 OLS 无须假设"严格外生性"与"球形扰动项"， 相比于小样本OLS，具有更大的适用性，两者所需的假设前提对比如下：

| 所需假设前提   | 小样本OLS                        | 大样本OLS                         |
| -------- | ----------------------------- | ------------------------------ |
| 方程形式     | 线性性（线性假定）                     | 线性性（线性假定）                      |
|          |                               | 渐近独立的平稳过程（所有变量渐近独立）            |
| 解释变量外生性质 | **严格外生性**（解释变量与**所有观测值**均不相关） | **前定解释变量**（解释变量与**同期扰动项**均不相关） |
| 变量间多重共线性 | 秩条件（不存在多重共线性）                 | 秩条件（不存在多重共线性）                  |
| 扰动项性质    | **球形扰动项**（扰动项同方差且不相关）         |                                |

在两者前提假设下，其具备的性质如下：

| 所具备的性质 | 小样本OLS            | 大样本OLS                |
| ------ | ----------------- | --------------------- |
| 1      |                   | 一致性（估计参数依概率收敛到真实参数）   |
| 2      |                   | 渐近正态性（估计量标准化后的分布趋近正态） |
| 3      | 无偏性（估计参数均值等于真实参数） |                       |
| 4      | 有效性（估计量是最佳线性无偏估计） |                       |

现实的数据千奇百怪，常不符合古典模型的某些假定，即使是上述大样本、小样本两种情况的讨论，尚不能完全覆盖全部情况。因此，接下来的章节，我们对违反经典OLS的特殊情况：异方差、自回归进行探讨。
# 异方差
当模型预测的**误差大小**（不确定性程度）**不是恒定的**，而是会随着解释变量的变化而变化，那么该特殊情况称为异方差。这种情况常出现在不同规模数据的情境下。
## 异方差的情景
"条件异方差"(conditional hete roskedasticity) ，简称**异方差**（Heteroskedasticity）指的是违背了**球型扰动项**假定中的**同方差**（Homoskedasticity）部分。这意味着扰动项 $\epsilon_i$ 的方差 $Var(\epsilon_i|X)$ 不再是常数 $\sigma^2$，而是依赖于观测值 $i$ 或解释变量 $X$。

异方差的后果：
1. OLS 估计量 $\hat{\beta}$ 仍满足**无偏性**、**一致性**、**渐近正态**（推导不依赖同方差）
2. OLS **标准误失效**：导致基于普通标准误的 t 检验、F 检验 不可靠。
3. OLS  **不再是最佳线性无偏估计**（有效性）：高斯-马尔可夫定理不再成立，OLS 不再是最高效的线性无偏估计量 。

因此，我们首先需要对异方差的情景进行检验（识别），再进行合适的处理。

## 异方差的检验方法
最直观的画残差图 (Residual Plots)方法，通过绘制残差与拟合值或解释变量的散点图，观察两者是否存在关联，从而识别异方差。虽然直观，但不严格、不准确，因此引入下面两种量化检验方法：
### BP检验
**BP 检验** (Breusch-Pagan Test)：检验扰动项方差是否与解释变量存在（通常假设为线性的）关系，构造 LM 或 F 统计量进行判断。
```Stata
estat hettest 
```
### 怀特检验
**怀特检验** (White Test)：BP 检验的扩展，通过在辅助回归中包含解释变量的二次项（含平方项与交叉项），检验更一般形式的异方差。
```Stata
estat imtest, white
```

## 异方差的处理方式 
1. 仍使用OLS 回归，但在检验时使用**稳健标准误**：最简单、通用的方法![[250420-计量经济学-1.png]]
2. 改用**加权最小二乘法**(WLS)：给予方差较小的观测值较大的权重后，最小化"加权残差平方和"

# 自相关
当模型预测的**误差项**在**不同时间点**（或不同观测单位）之间**存在关联**，即一个时期的误差会影响到下一时期的误差时，该特殊情况称为自相关。这种情况常出现在时间序列数据的情景下。

## 自相关的情景
**自相关**（Autocorrelation）或**序列相关**（Serial Correlation）指的是违背了**球型扰动项**假定中的**无自相关**（No Autocorrelation）部分。这意味着不同观测值的扰动项 $\epsilon_i$ 和 $\epsilon_j$ ($i \neq j$) 之间存在相关性，即 $Cov(\epsilon_i, \epsilon_j|X) \neq 0$。

自相关的后果（与异方差相同）：
1.  OLS 估计量 $\hat{\beta}$ 仍满足**无偏性**、**一致性**、**渐近正态**（推导不依赖无自相关）。
2.  OLS **标准误失效**：由于 $Var(\hat{\beta}|X)$ 的公式不再是 $\sigma^2(X'X)^{-1}$，导致基于普通标准误的 t 检验、F 检验不可靠。
3.  OLS **不再是最佳线性无偏估计**（有效性）：高斯-马尔可夫定理不再成立，OLS 不再是最高效的线性无偏估计量。

因此，我们首先需要对自相关的情景进行检验（识别），再进行合适的处理。

## 自相关的检验方法
最直观的仍然是画残差图：绘制**残差与残差滞后项**的散点图 (`scatter e L.e`)，直观判断是否存在线性关系，或绘制**残差自相关图** (Correlogram)，观察各阶自相关系数 (`ac e`) 是否显著异于零。其它量化方法有：
### BG检验
**BG 检验** (Breusch-Godfrey Test)：通过辅助回归检验残差与其滞后项是否存在关系（可检验高阶自相关，且允许解释变量非严格外生），构造 LM 统计量判断。
```Stata
estat bgodfrey 
```

### Q检验
 **Q 检验** (Portmanteau Test)：联合检验多个滞后阶数的自相关系数是否同时为零，常用 Ljung-Box Q 统计量。
```Stata
wntestq e 
```

### DW检验
**DW 检验** (Durbin-Watson Test)：较早的检验方法，**仅能检验一阶自相关**，且要求解释变量严格外生，存在无结论区域，**已不常用**。
```Stata
estat dwatson
```

### HAC检验
见下文处理方式1.

## 自相关的处理方式
1.  仍使用 OLS 回归，但在检验时使用 **HAC 稳健标准误** (Heteroskedasticity and Autocorrelation Consistent)：修正标准误以应对异方差和自相关，常用 Newey-West 方法。
    ```Stata
    newey y x1 x2 ... xk, lag(p)
    ```
2.  改用**可行广义最小二乘法** (FGLS)：通过变换原模型（如准差分法）消除自相关，得到更有效的估计，常用 Cochrane-Orcutt (CO) 或 Prais-Winsten (PW) 迭代法。
    ```Stata
    prais y x1 x2 ... xk [, corc nolog]
    ```
    (`corc` 选项指定使用 CO 方法，默认为 PW 方法；`nolog` 取消显示迭代过程)
3.  **修正模型设定**：自相关有时源于模型设定偏误（如遗漏了重要的滞后变量），可以尝试在模型中加入解释变量或被解释变量的滞后项（如VAR模型）

# 模型设定与数据问题
 前述章节的讨论是针对模型的参数估计问题，然而，在实验中，模型本身的设定与参数估计所使用的数据常常也不是完美的。本章节对其予以讨论
## 解释变量的选择
### 遗漏变量
当模型的解释变量未考虑某些因素时，则导致了变量的遗漏。常存在两种情况：
1. 遗漏变量与解释变量**不相关**：=>遗漏变量被归入扰动项中 =>影响OLS估计的准确度
2. 遗漏变量与解释变量**相关**：=>导致"遗漏变量偏差" (omitted variable bias) -> 是"致命伤"，需要尽量避免

其解决方法有：
1. 加入尽可能多的控制变量(control variable)
2. 工具变量法（见后文）
3. 使用面板数据（见后文）

### 无关变量
与遗漏变量相反的是，若模型的解释变量多考虑了某些因素，加入了与被解释变量 无关的变量，则称为无关变量。

变量的遗漏与过多考虑，均会导致模型不佳。为了选择合适的变量数目，引入如下的方法：
### 解决方式——信息准则
1. **AIC**"赤池信息准则"(Akaike Information Criterion)：选择解释变量的个数 K，使得目标函数最小化：$$
   \min_K \text{AIC} = \ln\left(\frac{\text{SSR}}{n}\right) + \frac{2}{n}K
$$
2. **BIC**"贝叶斯信息准则"(Bayesian Information Criterion) 或 "施瓦茨信息准则"(Schwarz Information Criterion, SIC 或 SBIC)：选择解释变量的个数 K，使得目标函数最小化：$$
   \min_K \text{BIC} = \ln\left(\frac{\text{SSR}}{n}\right) + \frac{\ln n}{n}K
$$

对比两类信息准则，其第一项为均对模型拟合度的奖励(减少残差平方和SSR)， 第二项为对解释变量过多的惩罚(为解释变量个数K的增函数)。BIC 准则更强调模型的简洁性。

这两类准则常用于选择解释变量，特别是在时间序列模型中，常用信息准则确定滞后阶数（如VAR模型的参数p）

## 多重共线性
 如果在解释变量中，某一**解释变量**可由其他**解释变量**线性表出， 则存在"严格多重共线性"(strict multicollinearity)。某个变量的方差膨胀因子（VIF）越大， 其**多重共线性问题越严重**，其**方差越大**。

## 虚拟变量
如何将定性数据纳入模型？可以通过引入**虚拟变量**，即取值为0或1的变量。

然而，如果定性指标共分M类，则最多只能在方程中放入**M-1**个虚拟变量，否则将导致**多重共线性**。

# 工具变量
前文提到，遗漏变量的解决方法之一是工具变量。当解释变量与扰动项相关，即存在**内生性**时，需要使用工具变量法。

## 工具变量法的原理
工具变量满足以下两个条件：
1. **相关性**(relevance)：工具变量与**内生解释变量**相关
2. **外生性**(exogeneity)：工具变量与**扰动项**不相关

常见的工具变量例子有：
1. 地理位置：研究受教育程度对工资的影响‌时，住在大学附近可能会影响一个人的受教育程度（一个内生解释变量），但不会直接影响工资水平、不与扰动项相关
2. 母亲的教育年限med、测试成绩kww：研究教育投资回报率时，母亲教育年限、测试成绩与智商iq（一个内生解释变量）相关，但不与扰动项相关
3. 降雨量：研究看电视是否引发小儿自闭症时，降雨越多的地区，人们呆在室内时间越长，看电视时间也越长， 故其与看电视相关，但不直接影响小儿自闭症的产生

## 实现方式——二阶段OLS

工具变量法一般通过“二阶段最小二乘法”(Two Stage Least Square，2SLS 或 TSLS)来实现，具体来说：
1. 第一阶段回归：用**内生解释变量**对**工具变量**回归，得到拟合值
2. 第二阶段回归：用**被解释变量**对**第一阶段回归的拟合值**进行回归 

# 面板数据
面板数据结合了截面数据和时间序列数据的特点，追踪多个个体在多个时间点上的信息。其核心优势在于能够控制**未观测到的个体异质性** (unobserved individual heterogeneity)，通常用 $a_i$ 表示，它代表不随时间改变但可能影响 $y_{it}$ 的个体特定因素（如个体能力、企业文化等）。

一个基本的面板数据模型可以写为：
$$y_{it} = x_{it}'\beta + a_i + u_{it} $$
其中，$y_{it}$ 是个体 $i$ 在时间 $t$ 的被解释变量，$x_{it}$ 是解释变量向量，$a_i$ 是个体固定效应，$u_{it}$ 是随个体和时间变化的扰动项。

处理 $a_i$ 的方式主要有两种模型：

1.  **固定效应 (Fixed Effects, FE)**：
    *   **核心假设**: 个体效应 $a_i$ 与解释变量 $x_{it}$ **相关**，即 $Cov(x_{it}, a_i) \neq 0$。 
    *   **解释**: 认为 $a_i$ 是每个个体的固定、待估计的参数。由于 $a_i$ 可能与 $x_{it}$ 相关，若不处理会导致遗漏变量偏差。FE 模型通过在估计过程中消除 $a_i$（例如，通过组内离差变换 demeaning）来获得对 $\beta$ 的一致估计。它关注的是个体**内部** (within) 的变动对 $y_{it}$ 的影响。
2.  **随机效应 (Random Effects, RE)**：
    *   **核心假设**:  个体效应 $a_i$ 与解释变量 $x_{it}$ **不相关**，即 $Cov(x_{it}, a_i) = 0$。
    *   **解释**: 认为 $a_i$ 是随机抽取的，如同扰动项的一部分。在此假设下，$a_i$ 不会引起遗漏变量偏差，可以直接将其视为复合扰动项 $v_{it} = a_i + u_{it}$ 的一部分。RE 模型利用个体**之间** (between) 和个体**内部** (within) 的信息进行估计，通常使用广义最小二乘法 (GLS) 来处理复合扰动项的序列相关问题，若假设成立则比 FE 更有效。

**选择**: 通常使用**豪斯曼检验 (Hausman Test)** 来判断 $a_i$ 与 $x_{it}$ 是否相关，从而在 FE 和 RE 之间做出选择。若检验结果拒绝"不相关"的原假设 ($Cov(x_{it}, a_i) = 0$)，则应使用固定效应模型。